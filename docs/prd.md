# CorindaGPT Product Requirements Document (PRD)

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-08-07 | 1.0 | Initial PRD draft creation. | John (PM) |

## 1. Goals and Background Context

### Goals

* To rewrite the existing CorindaGPT codebase into a new, high-performance version optimized for extremely low latency suitable for a live performance.
* To create a robust, real-time AI application that functions as a secret assistant to a magician, creating the illusion of an all-knowing AI.
* To implement a flexible, phase-based prompt management system using `langchain-core`, allowing the magician to easily modify the AI's personality and responses across five distinct performance phases.
* To build the core logic with a native Python `asyncio` pipeline to ensure minimal latency from voice input to text-to-speech output.
* To design a reliable state machine that interprets discreet inputs (e.g., short press, long press) from a single button on a custom microphone.
* To ensure the application can run effectively in a headless mode on limited hardware, such as a mini PC.
* To decouple the "magic" routines and externalize their coded language data, allowing the magician to update or change tricks without requiring code modifications.

### Background Context

CorindaGPT is a specialized AI application designed to function as a hidden accomplice in a live magic performance. The system creates the illusion of a hyper-intelligent AI by secretly listening to the magician's seemingly conversational but actually coded spoken phrases.

In real-time, the application transcribes the magician's voice, decodes the hidden meaning (e.g., a specific playing card, a name), and sends this secret data to a large language model (LLM). The LLM is prompted to generate a "miraculous" response that appears impossible to the audience. This response is then instantly converted to speech and played aloud, forming the climax of the illusion. The core design principles are stealth, speed, and reliability, as the system's success is paramount to the live performance.

## 2. Requirements

### Functional Requirements

1.  **FR1**: The system shall manage a configurable set of up to five performance phases, allowing the magician to select a specific sequence or a single phase to run for a given performance.
2.  **FR2**: The system must support a mechanism for transitioning between the configured performance phases. This trigger mechanism shall be adaptable to support either discreet physical inputs or keyword detection by the LLM.
3.  **FR3**: A phase transition shall load the prompt template and any specific logic or assets corresponding to the newly activated phase.
4.  **FR4**: The system shall support a "sustained input" pattern (e.g., press and hold) to activate a workflow that records audio, transcribes it to text, processes it through a decoder module, sends it to the LLM, and plays the TTS response.
5.  **FR5**: The system shall support a "brief input" pattern (e.g., a quick press) to trigger playback from an audio queue.
6.  **FR6**: The system shall support a "compound input" pattern (e.g., a double press and hold) to activate a workflow that records audio, transcribes it directly to text (bypassing the decoder), sends it to the LLM, and plays the TTS response.
    * *Note: The physical hardware for these input patterns (FR4, FR5, FR6) is to be determined and may include buttons, capacitive touch sensors, or pressure-sensitive sheets*.
7.  **FR7**: The system must use `langchain-core` for its `PromptTemplate` capabilities to manage AI prompts.
8.  **FR8**: The system's audio queue must function as a priority queue. Any new audio response generated by the LLM must be inserted at the front of the queue to be played next, ahead of any pre-loaded phrases.
9.  **FR9**: For specific phases (e.g., a "pull-string doll" routine), the audio queue shall be pre-loadable with a set of default audio phrases that can be configured to play sequentially or randomly upon receiving a "brief input" trigger.
10. **FR10**: The system shall maintain conversational memory/context and pass it reliably to the AI model during interactions.
11. **FR11**: Specialized decoder modules (e.g., `decode_hadley.py`) shall be used to process and extract specific information from transcribed text before sending it to the LLM.
12. **FR12**: The "coding system data" for magic routines must be externalized into user-editable data files (e.g., CSV, JSON) that are loaded by the corresponding decoder module.
13. **FR13**: All application configuration, including API keys, model names, and file paths, must be centralized into a single, structured YAML file.

### Non-Functional Requirements

1.  **NFR1**: The end-to-end latency from microphone input to speaker output must be extremely low, enabling the interaction to feel like a natural conversation during a live performance.
2.  **NFR2**: The application must perform reliably on a mini PC with limited processing power and memory.
3.  **NFR3**: The application must run in a headless mode without a graphical user interface.
4.  **NFR4**: The core logic for API interactions must be built using Python's `asyncio` and a high-performance HTTP client like `httpx` to minimize processing overhead.
5.  **NFR5**: The system must implement full end-to-end streaming, piping the response stream from the LLM API directly into the TTS API stream to reduce perceived latency.
6.  **NFR6**: The operation of the system must be stealthy and silent, making it invisible to the audience.

## 3. User Interface Design Goals

This section is not applicable as the project is designed for headless operation with no graphical user interface.

## 4. Technical Assumptions

### Repository Structure: Polyrepo

* A single repository will be used to house the entire application codebase.
* **Rationale**: The modules are tightly coupled and form a single, cohesive application. A single repository is the simplest and most effective structure for this type of project.

### Service Architecture: Monolith

* The application will be built as a single, monolithic process.
* **Rationale**: The system's logic is self-contained and designed to run as one unit. There is no requirement for distributed services, making a monolithic architecture the most direct and reliable approach.

### Testing Requirements: Unit + Integration

* The project will require both unit tests for individual modules and integration tests to verify the end-to-end asynchronous pipeline.
* **Rationale**: Given the critical need for reliability during a live performance, a comprehensive testing strategy is essential to ensure all components work together flawlessly and that individual logic is correct.

### Additional Technical Assumptions and Requests

* **Primary Language**: The application will be written in Python, specifically leveraging the `asyncio` library for the core execution pipeline.
* **Core Libraries**: Development will use `langchain-core` for prompt management and a high-performance library like `httpx` for direct, asynchronous API calls.
* **AI Service Provider**: The system will standardize on OpenAI's models (GPT-4/GPT-5), deprecating previous modular approaches for other LLMs.
* **Text-to-Speech Service**: The system will integrate with a third-party TTS service, such as ElevenLabs.
* **Deployment Target**: The application is intended to run on a mini PC with limited resources and in a headless environment.
* **Configuration**: All configuration will be managed via a single, centralized YAML file.

## 5. Epic List

1.  **Epic 1: Foundation & Core Pipeline Setup**
    * **Goal:** Establish the core application structure, configuration, and a working, low-latency `asyncio` pipeline from basic audio input to TTS output.
2.  **Epic 2: State Management & Phase-Based Prompts**
    * **Goal:** Implement the complete performance phase management system, including the state machine, dynamic prompt loading from external files, and phase transition logic.
3.  **Epic 3: Advanced Input Handling & Decoder Integration**
    * **Goal:** Implement the advanced input handler for discreet performance control and integrate the specialized decoder modules for translating the magician's coded phrases.
4.  **Epic 4: Audio Queue & Conversational Memory**
    * **Goal:** Implement a priority-based audio queue and robust conversational memory to enable advanced performance routines and maintain context.

## 6. Epic Details

### Epic 1: Foundation & Core Pipeline Setup

**Expanded Goal:** This first epic is the most critical as it establishes the project's foundation and proves the technical viability of the core requirement: a high-performance, low-latency pipeline. By the end of this epic, we will have a functioning, end-to-end `asyncio` application that can capture audio, send a request to the AI, and play back a spoken response, ensuring the fundamental architecture is sound before we add more complex logic.

---

#### **Story 1.1: Project Scaffolding & Configuration Management**

* **As a** Magician-Developer, **I want** a standardized project structure and a centralized YAML configuration file, **so that** I can easily manage settings, dependencies, and code organization for the new application.

##### Acceptance Criteria

1.  A Python project is initialized with a standard directory structure (e.g., `src`, `config`, `prompts`, `tests`).
2.  A `config.yaml` file is created containing placeholder values for all necessary API keys (OpenAI, ElevenLabs) and core settings (e.g., model names).
3.  An `initialization.py` module can successfully load and parse the `config.yaml` file, making all settings available to other modules within the application.
4.  All required core dependencies (e.g., `asyncio`, `httpx`, `langchain-core`, `PyYAML`) are defined in a `requirements.txt` file.

#### **Story 1.2: Basic Audio Input Capture**

* **As a** Magician-Developer, **I want** the application to listen for and capture audio from a microphone upon a specific keyboard input, **so that** my spoken commands can be recorded for processing.

##### Acceptance Criteria

1.  The `main.py` event loop can successfully detect a keyboard input (e.g., the F12 key being pressed and held).
2.  Upon detecting the input, the `voice_to_text.py` module is triggered and begins recording audio from the system's default microphone.
3.  The recording stops when the keyboard input is released.
4.  The captured audio shall be mono-channel and prepared in a format suitable for efficient, low-latency network transmission (e.g., an in-memory buffer for streaming or a compressed format like MP3).
5.  A confirmation message (e.g., "Audio captured") is printed to the console for verification.

#### **Story 1.3: Low-Latency Audio Transcription**

* **As a** Magician-Developer, **I want** the captured audio to be transcribed into text with the lowest possible latency, **so that** the AI can respond in near real-time.

##### Acceptance Criteria

1.  The `voice_to_text.py` module successfully integrates with a speech recognition library to process the captured audio.
2.  The transcription service provider (e.g., OpenAI Whisper or a lower-latency alternative) shall be configurable in `config.yaml`.
3.  The implementation must prioritize low latency by investigating and potentially implementing real-time audio streaming to the transcription service, as an alternative to sending a completed audio file.
4.  The resulting text transcript is successfully returned to the `main.py` event loop.
5.  The full transcript is printed to the console for verification.

#### **Story 1.4: End-to-End "Hello World" Pipeline**

* **As a** Magician-Developer, **I want** to trigger a hardcoded request to the LLM and have its response converted to speech and played back, **so that** the fundamental, end-to-end `asyncio` pipeline and its latency can be verified.

##### Acceptance Criteria

1.  The `gpt.py` module makes a successful asynchronous API call to OpenAI using `httpx` and the configured API key.
2.  The `text_to_speech.py` module makes a successful asynchronous API call to the chosen TTS service (e.g., ElevenLabs) using the configured API key.
3.  After receiving a transcript (from Story 1.3), the `main.py` loop triggers a simple, hardcoded request to the LLM (e.g., "Tell me a short, one-sentence joke").
4.  The text response from the LLM is passed directly to the TTS module.
5.  The resulting audio is played back through the system's default speakers.
6.  The entire process, from the LLM API call to the end of audio playback, is non-blocking and managed by the `asyncio` event loop.

### Epic 2: State Management & Phase-Based Prompts

**Expanded Goal:** This epic builds on the core pipeline by implementing the "brain" of the performance. By the end of this epic, the application will be able to manage a configurable sequence of performance phases, dynamically load different AI prompts for each phase using `langchain-core`, and transition between them. This gives the magician full, flexible control over the show's narrative flow.

---

#### **Story 2.1: Implement Core Application State Machine**

* **As a** Magician-Developer, **I want** a simple state machine to manage the application's operational status (e.g., IDLE, LISTENING, PROCESSING), **so that** the application can handle different operations sequentially and without conflicts.

##### Acceptance Criteria

1.  A state machine is implemented with at least three states: `IDLE`, `LISTENING`, and `PROCESSING`.
2.  The application correctly starts in the `IDLE` state.
3.  The audio capture logic (from Story 1.2) transitions the state from `IDLE` to `LISTENING`.
4.  After audio is successfully captured, the state transitions from `LISTENING` to `PROCESSING`.
5.  After the full response pipeline is complete (TTS audio finishes playing), the state returns to `IDLE`.
6.  The current state is logged to the console during every transition for debugging purposes.

#### **Story 2.2: Implement Performance Phase Management**

* **As a** Magician-Developer, **I want** the application to manage a configurable sequence of performance phases, **so that** I can customize which parts of the show to run for a specific performance.

##### Acceptance Criteria

1.  The application's core state is expanded to include `current_phase`, initialized to the first phase in the configured sequence.
2.  A new section is added to `config.yaml` to define the active sequence of phases (e.g., `performance_plan: [1, 3, 5]`).
3.  The application loads this `performance_plan` on startup.
4.  The system can correctly identify the current phase and determine the next phase based on the `performance_plan`.

#### **Story 2.3: Dynamic Prompt Loading with LangChain**

* **As a** Magician-Developer, **I want** the application to load different LLM prompts from external files based on the current performance phase, **so that** I can easily customize the AI's personality and behavior without changing any code.

##### Acceptance Criteria

1.  Five placeholder prompt files (e.g., `phase_1_prompt.txt`, `phase_2_prompt.txt`) are created in a `prompts` directory.
2.  The application uses `langchain-core`'s `PromptTemplate` to load and parse the prompt file that corresponds to the `current_phase`.
3.  The hardcoded prompt from Story 1.4 is now replaced with the dynamically loaded prompt.
4.  The system gracefully handles a missing prompt file by logging an error and using a default, generic fallback prompt.

#### **Story 2.4: Phase Transition Logic**

* **As a** Magician, **I want** to be able to transition to the next phase of the performance using a discreet input, **so that** I can control the show's progression live on stage.

##### Acceptance Criteria

1.  A specific, configurable input pattern (e.g., a long press held for 3 seconds) is defined to trigger a phase transition.
2.  When this input is detected, the application advances to the next phase as defined in the `performance_plan` from the configuration file.
3.  Upon transition, the `current_phase` state is updated, and the new prompt for that phase is immediately loaded.
4.  A confirmation of the phase change (e.g., "Phase transitioned to 3") is logged to the console.
5.  If the last phase in the `performance_plan` is reached, subsequent transition triggers will not change the phase further.

### Epic 3: Advanced Input Handling & Decoder Integration

**Expanded Goal:** This epic implements the mechanics of the illusion. It builds upon the state machine by introducing the complex, multi-pattern input handler that allows the magician to discreetly control the application with nuanced actions. It also integrates the specialized decoder modules that translate the magician's secret coded language into actionable data for the LLM.

---

#### **Story 3.1: Develop Abstract Input Handler**

* **As a** Magician-Developer, **I want** a dedicated input handler that can distinguish between different physical input patterns (brief, sustained, compound), **so that** the application can trigger different actions from a single input source.

##### Acceptance Criteria

1.  A new input handler module is created that can reliably detect and differentiate between a "brief input" (e.g., a quick press), a "sustained input" (e.g., press and hold), and a "compound input" (e.g., double press and hold).
2.  The time durations that define these patterns (e.g., the threshold for a "sustained" press) are configurable in `config.yaml`.
3.  The `main.py` event loop is refactored to receive abstract events from this handler (e.g., `BRIEF_INPUT_DETECTED`) instead of directly checking for keyboard presses.
4.  The handler is designed to be independent of the physical hardware, allowing for future integration with custom sensors or buttons without changing the core application logic.

#### **Story 3.2: Integrate Input Patterns with Core Workflows**

* **As a** Magician-Developer, **I want** to connect the detected input patterns to the core application workflows, **so that** I can control the performance with specific actions.

##### Acceptance Criteria

1.  A "sustained input" event from the handler correctly triggers the full record-transcribe-decode-LLM-TTS workflow.
2.  A "brief input" event from the handler correctly triggers the playback of the next item from the audio queue.
3.  A "compound input" event from the handler correctly triggers the record-transcribe-LLM-TTS workflow that bypasses the decoder module.
4.  The phase transition logic (from Story 2.4) is successfully integrated with its corresponding input pattern in the handler.

#### **Story 3.3: Implement Decoder Module Integration**

* **As a** Magician-Developer, **I want** to integrate a specialized decoder module into the main processing workflow, **so that** my coded phrases can be translated into secret information for the LLM.

##### Acceptance Criteria

1.  A placeholder decoder module (e.g., `decode_hadley.py`) is created with a function that accepts a text string and returns a processed text string.
2.  The main application logic is updated to call this decoder on the transcribed text before the text is passed to the `PromptTemplate`.
3.  The workflow triggered by a "sustained input" correctly uses the decoder's output.
4.  The workflow triggered by a "compound input" correctly bypasses the decoder, sending the raw transcript to the prompt.

#### **Story 3.4: Externalize Decoder Data**

* **As a** Magician, **I want** the data for my coded language system to be stored in an external file, **so that** I can modify the codes for a trick without needing to change the Python code.

##### Acceptance Criteria

1.  A sample data file (e.g., `hadley_codes.json`) is created containing key-value pairs that represent a coded language system.
2.  The decoder module (from Story 3.3) is refactored to load and use this external data file to perform its translation.
3.  The file path for the decoder's data file is configurable in `config.yaml`.
4.  The decoder successfully translates a coded phrase from a transcript into its corresponding secret data by looking it up in the external file.

### Epic 4: Audio Queue & Conversational Memory

**Expanded Goal:** This final epic adds the layers of sophistication required for a seamless and dynamic performance. It implements the priority-based audio queue, enabling complex interactions like the "pull-string doll" routine. It also introduces conversational memory, allowing the AI assistant to recall previous interactions and make the illusion of an all-knowing entity more convincing.

---

#### **Story 4.1: Implement a Basic First-In, First-Out Audio Queue**

* **As a** Magician-Developer, **I want** a basic audio queue to store and play back generated TTS audio files sequentially, **so that** multiple AI responses can be handled in the order they were received without overlapping.

##### Acceptance Criteria

1.  An `audio_queue.py` module is created that manages a standard queue (First-In, First-Out) of audio file paths.
2.  When the `text_to_speech.py` service successfully generates a new audio file, its path is added to the end of this queue.
3.  The "brief input" event (from Story 3.2) is confirmed to trigger a `play_next()` function in the audio queue module.
4.  The `play_next()` function plays the audio file at the front of the queue and removes it upon successful playback.
5.  If the queue is empty when `play_next()` is called, a pre-programmed default phrase is played (as per FR5).

#### **Story 4.2: Enhance Audio Queue with Priority & Pre-loading**

* **As a** Magician, **I want** the audio queue to prioritize new LLM responses and support pre-loaded phrases for specific routines, **so that** I can perform advanced effects like the "pull-string doll".

##### Acceptance Criteria

1.  The audio queue logic is refactored to function as a priority queue.
2.  Newly generated audio from an LLM response is always inserted at the *front* of the queue, making it the next item to be played.
3.  A mechanism is created to pre-load the queue with a list of default audio file paths defined in `config.yaml`.
4.  The pre-loaded phrases can be configured to be added to the queue in either sequential or random order.
5.  The "brief input" trigger correctly plays whichever audio file is at the front of the queue, whether it is a prioritized LLM response or a pre-loaded phrase.

#### **Story 4.3: Implement Conversational Memory**

* **As a** Magician-Developer, **I want** the application to maintain a history of the conversation with the LLM, **so that** the AI can provide contextually aware responses and remember previous interactions.

##### Acceptance Criteria

1.  A `conversational_memory.py` module is created to store a history of user inputs (transcripts) and the AI's corresponding text responses.
2.  The number of past conversation turns to remember is configurable in `config.yaml`.
3.  Before sending a new request to the LLM, the `gpt.py` module retrieves the current conversation history.
4.  The conversation history is correctly formatted and included as part of the context within the `PromptTemplate` sent to the LLM.
5.  After a successful LLM interaction, the latest user input and AI response are added to the memory.

## 7. Checklist Results Report

The `pm-checklist` has been executed against the Product Requirements Document. The PRD is comprehensive, logically sequenced, and appropriately scoped for an MVP. It is ready for the architecture phase.

#### **Category Statuses**

| Category | Status | Critical Issues |
| :--- | :--- | :--- |
| 1. Problem Definition & Context | ✅ PASS | None |
| 2. MVP Scope Definition | ✅ PASS | None |
| 3. User Experience Requirements | N/A | Project is headless. |
| 4. Functional Requirements | ✅ PASS | None |
| 5. Non-Functional Requirements | ✅ PASS | None |
| 6. Epic & Story Structure | ✅ PASS | None |
| 7. Technical Guidance | ✅ PASS | None |
| 8. Cross-Functional Requirements | ✅ PASS | None |
| 9. Clarity & Communication | ✅ PASS | None |